{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveFormula(nn.Module):\n",
    "    \"\"\"\n",
    "    Class used for representing formulas\n",
    "    \n",
    "    Attributes:\n",
    "        depth\n",
    "        num_variables\n",
    "        powers\n",
    "        lambdas - list of linear coefficients\n",
    "        subformulas - list of subformulas of smaller depth, which are used for computing\n",
    "        parameters - list of all learnable parameters of formula\n",
    "        last_subformula - additional subformula, which is not multiplied by variable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, depth=0, num_variables=1):\n",
    "        super(RecursiveFormula, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.num_variables = num_variables\n",
    "        self.powers = []\n",
    "        self.lambdas = []\n",
    "        self.subformulas = []\n",
    "        self.parameters = []\n",
    "        # When depth is zero, formula is just a real number\n",
    "        if depth == 0:\n",
    "            new_lambda = (2 * torch.randn(1, 1)).requires_grad_(True)\n",
    "            self.lambdas.append(new_lambda)\n",
    "            self.parameters.append(new_lambda)\n",
    "        else:\n",
    "            for i in range(self.num_variables):\n",
    "                # When depth is 1, we do not need to create subformulas, since they would be just real numbers\n",
    "                if self.depth != 1:\n",
    "                    subformula = RecursiveFormula(self.depth - 1, self.num_variables)\n",
    "                    self.subformulas.append(subformula)\n",
    "                    self.parameters.extend(subformula.parameters)\n",
    "                new_lambda = (2 * torch.randn(1, 1)).requires_grad_(True)\n",
    "                new_power = (2 * torch.randn(1, 1)).requires_grad_(True)\n",
    "                self.lambdas.append(new_lambda)\n",
    "                self.powers.append(new_power)\n",
    "                self.parameters.extend([new_power, new_lambda])\n",
    "            subformula = RecursiveFormula(self.depth - 1, self.num_variables)\n",
    "            self.last_subformula = subformula\n",
    "            self.parameters.extend(subformula.parameters)\n",
    "                                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Iterate over subformulas, recursively computing result using results of subformulas\n",
    "        \"\"\"\n",
    "        # When depth is 0, we just return the corresponding number\n",
    "        if self.depth == 0:\n",
    "            return torch.full((x.shape[0], 1), self.lambdas[0].item())\n",
    "        \n",
    "        ans = torch.zeros(x.shape[0], 1)\n",
    "        for i in range(self.num_variables):\n",
    "            x_powered = torch.t(x[:, i]**self.powers[i])\n",
    "            subformula_result = torch.ones((x.shape[0], 1))\n",
    "            # When depth is 1, we do not need to compute subformulas\n",
    "            if self.depth != 1:\n",
    "                subformula_result = self.subformulas[i](x)\n",
    "            assert subformula_result.shape == (x.shape[0], 1)\n",
    "            assert x_powered.shape == (x.shape[0], 1)\n",
    "            ans += self.lambdas[i] * x_powered * subformula_result\n",
    "            \n",
    "        ans += self.last_subformula(x)\n",
    "        return ans\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return tex-style string, recursively combining result from representation of subformulas\n",
    "        \"\"\"\n",
    "        if self.depth == 0:\n",
    "            return \"{:.3}\".format(self.lambdas[0].item())\n",
    "        \n",
    "        ans = [\"(\"]\n",
    "        for i in range(self.num_variables):\n",
    "            if i != 0 and self.lambdas[i] > 0:\n",
    "                ans.append(\" + {:.3}\".format(self.lambdas[i].item()))\n",
    "            else:\n",
    "                ans.append(\"{:.3}\".format(self.lambdas[i].item()))            \n",
    "            ans.append(\"x_{}^\".format(i + 1) + \"{\" + \"{:.3}\".format(self.powers[i].item()) + \"}\")\n",
    "            if self.depth != 1:\n",
    "                ans.append(str(self.subformulas[i]))\n",
    "        if self.last_subformula.lambdas[0] > 0:        \n",
    "            ans.append(\" + \")\n",
    "        ans.append(str(self.last_subformula))\n",
    "        ans.append(\")\")\n",
    "        ans = ''.join(ans)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(formula):\n",
    "    print(\"depth: {}, number of variables: {}, total parameters: {}\".format(\n",
    "        formula.depth, formula.num_variables, len(formula.parameters)))\n",
    "    \n",
    "def PrintFormula(formula, mode=\"slow\"):\n",
    "#     info(network)\n",
    "    if mode == \"slow\":\n",
    "        display(Math(str(formula)))   \n",
    "    else:\n",
    "        print(formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение\n",
    "\n",
    "Пока модель быстро входит в локальный экстремум, и результат сильно зависит от начальной инициализации весов. До последнего свободного члена градиент не доходит. Буду пробовать разные оптимизаторы и гиперпараметры. Возможно, параметры powers и lambdas имеют разную природу, и надо их регуляризовать по отдельности с разными коэффициентами, которые тоже надо будет подобрать. Модели может быть выгодно просто делать степени большими по модулю отрицательными чистами. Еще надо добавить обучение по батчам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LearnFormula(X, y, depth=1, num_epochs=3000, verbose=True):\n",
    "    formula = RecursiveFormula(depth, X.shape[1])\n",
    "    # create your optimizer\n",
    "    optimizer = optim.Rprop(formula.parameters)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = formula(X)\n",
    "        loss = criterion(output, y) \n",
    "        loss.backward()\n",
    "        if verbose and (epoch + 1) % 200 == 0:\n",
    "            print(\"Epoch {}, current loss {:.3}, current formula \".format(epoch + 1, loss.item()), end='')\n",
    "            PrintFormula(formula, \"fast\")       \n",
    "        optimizer.step()  \n",
    "    return formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = torch.rand(100, 1) * 10\n",
    "y1 = 2.5 * X1**2 + 3\n",
    "\n",
    "X2 = torch.rand(100, 2) * 10\n",
    "y2 = 2.5 * X2[:, 0]**2 + 0.333 * X2[:, 1]**0.5 + 3    \n",
    "# y = 1.2 * X[:, 0]**2.1 * X[:, 2] + 2.5 * X[:, 1]**(-3) + 1/2 * X[:, 2]**0.3333 * X[:, 1]**(-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, current loss 8.55, current formula (2.14x_1^{2.08} + 0.968)\n",
      "Epoch 400, current loss 1.73, current formula (2.58x_1^{1.99} + 0.968)\n",
      "Epoch 600, current loss 1.03, current formula (2.73x_1^{1.96} + 0.968)\n",
      "Epoch 800, current loss 0.952, current formula (2.78x_1^{1.96} + 0.968)\n",
      "Epoch 1000, current loss 0.948, current formula (2.8x_1^{1.95} + 0.968)\n",
      "Epoch 1200, current loss 0.947, current formula (2.8x_1^{1.95} + 0.968)\n",
      "Epoch 1400, current loss 0.947, current formula (2.8x_1^{1.95} + 0.968)\n",
      "Epoch 1600, current loss 0.947, current formula (2.8x_1^{1.95} + 0.968)\n",
      "Epoch 1800, current loss 0.947, current formula (2.8x_1^{1.95} + 0.968)\n",
      "Epoch 2000, current loss 0.947, current formula (2.8x_1^{1.95} + 0.968)\n",
      "Epoch 2200, current loss 0.947, current formula (2.8x_1^{1.95} + 0.968)\n",
      "Epoch 2400, current loss 0.947, current formula (2.8x_1^{1.95} + 0.968)\n",
      "Epoch 2600, current loss 0.947, current formula (2.8x_1^{1.95} + 0.968)\n",
      "Epoch 2800, current loss 0.947, current formula (2.8x_1^{1.95} + 0.968)\n",
      "Epoch 3000, current loss 0.947, current formula (2.8x_1^{1.95} + 0.968)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle (2.8x_1^{1.95} + 0.968)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PrintFormula(LearnFormula(X1, y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{8.79e-06} + 2.81)\n",
      "Epoch 400, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 600, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 800, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 1000, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 1200, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 1400, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 1600, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 1800, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 2000, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 2200, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 2400, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 2600, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 2800, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n",
      "Epoch 3000, current loss 6.11e+03, current formula (85.2x_1^{6.77e-08} + 6.85x_2^{6.8e-06} + 2.81)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle (85.2x_1^{1.07e-06} + 6.85x_2^{7.8e-06} + 2.81)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PrintFormula(LearnFormula(X2, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
