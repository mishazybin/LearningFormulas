{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q41R-POjCq1e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import MSELoss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_length_of_fraction(numerator, denominator):\n",
    "    return torch.log2((1 + torch.abs(torch.tensor(denominator))) * torch.abs(numerator))\n",
    "\n",
    "\n",
    "def logplus(number):\n",
    "    return torch.log2(1 + number**2) / 2\n",
    "\n",
    "\n",
    "def descriptive_length_of_real_number(real_number, precision_floor=1e-8):\n",
    "    return logplus(real_number / torch.tensor(precision_floor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FormFractionRepresentation(fraction: torch.tensor) -> str:\n",
    "    if fraction[1].item() != 1:\n",
    "        return r\"\\frac{\" + str(int(fraction[0].item())) + \"}{\" + str(int(fraction[1].item())) + \"}\"\n",
    "    return str(int(fraction[0].item()))\n",
    "\n",
    "\n",
    "def FormReal(number: torch.tensor) -> str:\n",
    "    return \"{:.3}\".format(number.item())\n",
    "\n",
    "\n",
    "def AddRationalInName(name: str) -> str:\n",
    "    if 'lambda' in name:\n",
    "        position = name.find('lambda')\n",
    "    else:\n",
    "        position = name.find('power')\n",
    "    return name[:position] + \"rational_\" + name[position:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMZsInkrCq1w"
   },
   "outputs": [],
   "source": [
    "def info(formula):\n",
    "    print(\"depth: {}, number of variables: {}, total parameters: {}\".format(\n",
    "        formula.depth, formula.num_variables, len(formula.parameters)))\n",
    "    \n",
    "def PrintFormula(formula, mode=\"slow\"):\n",
    "#     info(network)\n",
    "    if mode == \"slow\":\n",
    "        display(Math(str(formula)))   \n",
    "    else:\n",
    "        print(formula)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9aiZJrHCq1p"
   },
   "outputs": [],
   "source": [
    "class RecursiveFormula(nn.Module):\n",
    "    \"\"\"\n",
    "    Class used for representing formulas\n",
    "    \n",
    "    Attributes:\n",
    "        depth\n",
    "        num_variables\n",
    "        powers\n",
    "        lambdas - list of linear coefficients\n",
    "        subformulas - list of subformulas of smaller depth, which are used for computing\n",
    "        parameters - list of all learnable parameters of formula (real numbers)\n",
    "        rational_values - if during simplification it turns out that some parameter is actually a rational number, \n",
    "                            its value is stored here as a tuple (numerator, denominator), otherwise there is None\n",
    "        last_subformula - additional subformula, which is not multiplied by variable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, depth=0, num_variables=1):\n",
    "        super(RecursiveFormula, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.num_variables = num_variables\n",
    "        self.num_parameters = 0\n",
    "        self.powers = []\n",
    "        self.lambdas = []\n",
    "        self.subformulas = nn.ModuleList()\n",
    "#         self.parameters = []\n",
    "        # When depth is zero, formula is just a real number\n",
    "        if depth == 0:\n",
    "            new_lambda = nn.Parameter((2 * torch.randn((1, 1)))).requires_grad_(True)\n",
    "            self.lambdas.append(new_lambda)\n",
    "            self.register_parameter(\"lambda_0\", new_lambda)\n",
    "            new_rational_lambda = nn.Parameter(torch.tensor([0., 0.])).requires_grad_(False)\n",
    "            self.register_parameter(\"rational_lambda_0\", new_rational_lambda)\n",
    "            self.num_parameters += 1\n",
    "#             self.parameters.append(new_lambda)\n",
    "        else:\n",
    "            for i in range(self.num_variables):\n",
    "                # When depth is 1, we do not need to create subformulas, since they would be just real numbers\n",
    "                if self.depth != 1:\n",
    "                    subformula = RecursiveFormula(self.depth - 1, self.num_variables)\n",
    "                    self.subformulas.append(subformula)\n",
    "                    self.num_parameters += subformula.num_parameters\n",
    "#                     self.parameters.extend(subformula.parameters)                    \n",
    "                new_lambda = nn.Parameter((2 * torch.randn((1, 1)))).requires_grad_(True)\n",
    "                new_power = nn.Parameter((2 * torch.randn((1, 1)))).requires_grad_(True)\n",
    "#                 new_rational_lambda = nn.Parameter(torch.round(new_lambda)).requires_grad_(False)\n",
    "#                 new_rational_power = nn.Parameter(torch.round(new_power)).requires_grad_(False)\n",
    "                new_rational_lambda = nn.Parameter(torch.tensor([0., 0.])).requires_grad_(False)\n",
    "                new_rational_power = nn.Parameter(torch.tensor([0., 0.])).requires_grad_(False)\n",
    "                self.register_parameter(\"lambda_{}\".format(i), new_lambda)\n",
    "                self.register_parameter(\"power_{}\".format(i), new_power)\n",
    "                self.register_parameter(\"rational_lambda_{}\".format(i), new_rational_lambda)\n",
    "                self.register_parameter(\"rational_power_{}\".format(i), new_rational_power)\n",
    "                self.lambdas.append(new_lambda)\n",
    "                self.powers.append(new_power)\n",
    "                self.num_parameters += 2\n",
    "#                 self.parameters.extend([new_lambda, new_power])\n",
    "            self.last_subformula = RecursiveFormula(self.depth - 1, self.num_variables)\n",
    "            self.num_parameters += self.last_subformula.num_parameters\n",
    "#             self.parameters.extend(self.last_subformula.parameters)\n",
    "        # Before simplification we assume that all parameters are real\n",
    "        self.rational_lambdas = [None for lambd in self.lambdas]\n",
    "        self.rational_powers = [None for power in self.powers]\n",
    "        \n",
    "                                    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Iterate over subformulas, recursively computing result using results of subformulas\n",
    "        \"\"\"\n",
    "        # When depth is 0, we just return the corresponding number\n",
    "        if self.depth == 0:\n",
    "            return self.lambdas[0].repeat(x.shape[0], 1)\n",
    "        \n",
    "        ans = torch.zeros(x.shape[0], 1)\n",
    "        for i in range(self.num_variables):\n",
    "            x_powered = torch.t(x[:, i]**self.powers[i])\n",
    "            subformula_result = torch.ones((x.shape[0], 1))\n",
    "            # When depth is 1, we do not need to compute subformulas\n",
    "            if self.depth != 1:\n",
    "                subformula_result = self.subformulas[i](x)\n",
    "            assert subformula_result.shape == (x.shape[0], 1)\n",
    "            assert x_powered.shape == (x.shape[0], 1)\n",
    "            ans += self.lambdas[i] * x_powered * subformula_result           \n",
    "        ans += self.last_subformula(x)\n",
    "        return ans\n",
    "    \n",
    "    def simplify(self, X_val, y_val, max_denominator=1, inplace=False):\n",
    "        simplified_version = copy.deepcopy(self)  \n",
    "        simplified_state_dict = simplified_version.state_dict()\n",
    "        \n",
    "        # Iterate over all parameters, try to substitute them with some rational number.\n",
    "        for key, value in self.state_dict().items():\n",
    "            if \"rational\" not in key: # We do not simplify rational parameters - they will be the result of simplification\n",
    "                simplified_version_for_iteration = copy.deepcopy(simplified_version)\n",
    "                simplified_state_dict_for_iteration = simplified_version_for_iteration.state_dict()\n",
    "                y_predict = simplified_version(X_val)\n",
    "                loss = MSELoss()(y_val, y_predict)\n",
    "                descriptive_length_of_loss = descriptive_length_of_real_number(loss)\n",
    "                descriptive_length_of_existing_parameter = descriptive_length_of_real_number(value)\n",
    "\n",
    "                # Iterate over all possible denominators\n",
    "                for possible_denominator in range(1, max_denominator + 1):\n",
    "                    print(\"trying denominator\", possible_denominator)\n",
    "                    simplified_parameter_numerator = torch.round(value * possible_denominator)\n",
    "                    simplified_state_dict_for_iteration[key] = simplified_parameter_numerator / possible_denominator\n",
    "                    simplified_version_for_iteration.load_state_dict(simplified_state_dict_for_iteration)\n",
    "                    descriptive_length_of_simplified_parameter = descriptive_length_of_fraction(simplified_parameter_numerator, possible_denominator)\n",
    "                    print(simplified_parameter_numerator, possible_denominator)\n",
    "                    y_predict_simplified = simplified_version_for_iteration(X_val)\n",
    "                    loss_of_simplified_model = MSELoss()(y_val, y_predict_simplified)\n",
    "                    descriptive_length_of_loss_of_simplified_model = descriptive_length_of_real_number(loss_of_simplified_model)                \n",
    "                    # If the descriptive length did not improve, revert the change.\n",
    "                    print(\"descriptive_length_of_loss_of_simplified_model\", descriptive_length_of_loss_of_simplified_model)\n",
    "                    print(\"descriptive_length_of_simplified_parameter\", descriptive_length_of_simplified_parameter)\n",
    "                    print(\"descriptive_length_of_loss\", descriptive_length_of_loss)\n",
    "                    print(\"descriptive_length_of_existing_parameter\", descriptive_length_of_existing_parameter)\n",
    "\n",
    "                    if descriptive_length_of_loss_of_simplified_model + descriptive_length_of_simplified_parameter > descriptive_length_of_loss + descriptive_length_of_existing_parameter:\n",
    "                        simplified_version_for_iteration.load_state_dict(simplified_state_dict)\n",
    "                    else:\n",
    "                        # If we are successful, we update everything\n",
    "                        print(key)\n",
    "                        simplified_state_dict[AddRationalInName(key)] = torch.tensor([simplified_parameter_numerator, possible_denominator])\n",
    "                        simplified_version.load_state_dict(simplified_state_dict)\n",
    "                        simplified_version_for_iteration = copy.deepcopy(simplified_version)\n",
    "                        simplified_state_dict_for_iteration = simplified_version_for_iteration.state_dict()\n",
    "\n",
    "                simplified_state_dict = simplified_state_dict_for_iteration\n",
    "                simplified_version.load_state_dict(simplified_state_dict)\n",
    "        \n",
    "        if inplace:\n",
    "            self = copy.deepcopy(simplified_version)\n",
    "        else:\n",
    "            return simplified_version      \n",
    "    \n",
    "    def get_lambda(self, i):\n",
    "        return self.state_dict()['lambda_{}'.format(i)]\n",
    "    \n",
    "    def get_rational_lambda(self, i):\n",
    "        return self.state_dict()['rational_lambda_{}'.format(i)]\n",
    "    \n",
    "    def get_power(self, i):\n",
    "        return self.state_dict()['power_{}'.format(i)]\n",
    "    \n",
    "    def get_rational_power(self, i):\n",
    "        return self.state_dict()['rational_power_{}'.format(i)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return tex-style string, recursively combining result from representation of subformulas\n",
    "        \"\"\"\n",
    "        if self.depth == 0:\n",
    "            if self.get_rational_lambda(0)[1] > 0: # if it is equal to 0, it means that there is no rational value\n",
    "                return FormFractionRepresentation(self.get_rational_lambda(0))            \n",
    "            return FormReal(self.get_lambda(0))\n",
    "        \n",
    "        ans = [\"(\"]\n",
    "        for i in range(self.num_variables):\n",
    "            # First we add lambda\n",
    "            if i != 0 and self.get_lambda(i) > 0:\n",
    "                ans.append(\" + \")\n",
    "            if self.get_rational_lambda(i)[1] > 0:\n",
    "                ans.append(FormFractionRepresentation(self.get_rational_lambda(i)))\n",
    "            else:\n",
    "                ans.append(FormReal(self.get_lambda(i)))   \n",
    "            # Then we add variable and its power\n",
    "            ans.append(\"x_{}^\".format(i + 1) + \"{\")\n",
    "            if self.get_rational_power(i)[1] > 0:\n",
    "                ans.append(FormFractionRepresentation(self.get_rational_power(i)))\n",
    "            else:\n",
    "                ans.append(FormReal(self.get_power(i)))  \n",
    "            ans += \"}\"    \n",
    "            # Then we add the corresponding subformula\n",
    "            if self.depth != 1:\n",
    "                ans.append(str(self.subformulas[i]))\n",
    "        if self.last_subformula.lambdas[0] > 0:        \n",
    "            ans.append(\" + \")\n",
    "        ans.append(str(self.last_subformula))\n",
    "        ans.append(\")\")\n",
    "        ans = ''.join(ans)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aTTheLOCq17"
   },
   "outputs": [],
   "source": [
    "def LearnFormula(X, y, n_init=1, max_iter=2000, depth=2, verbose=2, max_epochs_without_improvement=500,\n",
    "                minimal_acceptable_improvement=3e-6):\n",
    "    best_model = RecursiveFormula(depth, X.shape[1])\n",
    "    best_loss = 1e10\n",
    "    for initiation in range(n_init):\n",
    "    #     torch.random.manual_seed(seed)\n",
    "        formula = RecursiveFormula(depth, X.shape[1])\n",
    "        # create your optimizer\n",
    "        optimizer = optim.Rprop(formula.parameters(), lr=1e-3)\n",
    "        criterion = nn.MSELoss()\n",
    "        epochs_without_improvement = 0\n",
    "        epoch = 0\n",
    "        optimizer.zero_grad()\n",
    "        output = formula(X)\n",
    "        previous_loss = criterion(output, y).item() \n",
    "        while epoch < max_iter and epochs_without_improvement < max_epochs_without_improvement:\n",
    "            optimizer.zero_grad()\n",
    "            output = formula(X)\n",
    "            loss = criterion(output, y) \n",
    "            loss.backward()\n",
    "            if verbose and (epoch + 1) % 500 == 0:\n",
    "                print(\"Epoch {}, current loss {:.3}, current formula \".format(epoch + 1, loss.item()), end='')\n",
    "                PrintFormula(formula, \"fast\")       \n",
    "            optimizer.step()  \n",
    "            epoch += 1\n",
    "            if torch.abs(previous_loss - loss) < minimal_acceptable_improvement:\n",
    "                epochs_without_improvement += 1\n",
    "#                 print(torch.abs(previous_loss - loss))\n",
    "            else:\n",
    "                epochs_without_improvement = 0\n",
    "            previous_loss = loss.item()\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_formula = formula\n",
    "    return best_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_-H5b-ICq2A"
   },
   "outputs": [],
   "source": [
    "X1 = torch.rand(100, 1) * 10\n",
    "y1 = 2.5 * X1**2 + 3\n",
    "\n",
    "X2 = torch.rand(100, 2) * 10\n",
    "y2 = 2.5 * X2[:, 0]**2 + 0.333 * X2[:, 1]**0.5 + 3    \n",
    "# y = 1.2 * X[:, 0]**2.1 * X[:, 2] + 2.5 * X[:, 1]**(-3) + 1/2 * X[:, 2]**0.3333 * X[:, 1]**(-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, current loss 0.167, current formula (1.63x_1^{0.88}(3.05x_1^{-0.464}-3.76) + (3.9x_1^{1.86} + 3.14))\n",
      "Epoch 1000, current loss 0.0695, current formula (1.63x_1^{0.977}(3.44x_1^{-0.43}-3.75) + (3.9x_1^{1.87} + 2.46))\n",
      "Epoch 1500, current loss 0.0378, current formula (1.63x_1^{1.03}(3.68x_1^{-0.429}-3.75) + (3.9x_1^{1.87} + 2.05))\n",
      "Epoch 2000, current loss 0.0241, current formula (1.63x_1^{1.06}(3.82x_1^{-0.429}-3.75) + (3.9x_1^{1.88} + 1.77))\n"
     ]
    }
   ],
   "source": [
    "formula = LearnFormula(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying denominator 1\n",
      "tensor([[2.]]) 1\n",
      "descriptive_length_of_loss_of_simplified_model tensor(31.4934, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_simplified_parameter tensor([[2.]])\n",
      "descriptive_length_of_loss tensor(21.1975, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_existing_parameter tensor([[27.2779]])\n",
      "lambda_0\n",
      "trying denominator 1\n",
      "tensor([[1.]]) 1\n",
      "descriptive_length_of_loss_of_simplified_model tensor(29.4192, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_simplified_parameter tensor([[1.]])\n",
      "descriptive_length_of_loss tensor(21.1975, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_existing_parameter tensor([[26.6554]])\n",
      "power_0\n",
      "trying denominator 1\n",
      "tensor([[4.]]) 1\n",
      "descriptive_length_of_loss_of_simplified_model tensor(26.2320, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_simplified_parameter tensor([[3.]])\n",
      "descriptive_length_of_loss tensor(21.1975, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_existing_parameter tensor([[28.5086]])\n",
      "subformulas.0.lambda_0\n",
      "trying denominator 1\n",
      "tensor([[-0.]]) 1\n",
      "descriptive_length_of_loss_of_simplified_model tensor(35.8293, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_simplified_parameter tensor([[-inf]])\n",
      "descriptive_length_of_loss tensor(21.1975, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_existing_parameter tensor([[25.3546]])\n",
      "subformulas.0.power_0\n",
      "trying denominator 1\n",
      "tensor([[-4.]]) 1\n",
      "descriptive_length_of_loss_of_simplified_model tensor(29.5086, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_simplified_parameter tensor([[3.]])\n",
      "descriptive_length_of_loss tensor(21.1975, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_existing_parameter tensor([[28.4840]])\n",
      "subformulas.0.last_subformula.lambda_0\n",
      "trying denominator 1\n",
      "tensor([[4.]]) 1\n",
      "descriptive_length_of_loss_of_simplified_model tensor(30.3689, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_simplified_parameter tensor([[3.]])\n",
      "descriptive_length_of_loss tensor(21.1975, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_existing_parameter tensor([[28.5391]])\n",
      "last_subformula.lambda_0\n",
      "trying denominator 1\n",
      "tensor([[2.]]) 1\n",
      "descriptive_length_of_loss_of_simplified_model tensor(37.4382, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_simplified_parameter tensor([[2.]])\n",
      "descriptive_length_of_loss tensor(21.1975, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_existing_parameter tensor([[27.4842]])\n",
      "last_subformula.power_0\n",
      "trying denominator 1\n",
      "tensor([[2.]]) 1\n",
      "descriptive_length_of_loss_of_simplified_model tensor(22.8522, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_simplified_parameter tensor([[2.]])\n",
      "descriptive_length_of_loss tensor(21.1975, grad_fn=<DivBackward0>)\n",
      "descriptive_length_of_existing_parameter tensor([[27.4014]])\n",
      "last_subformula.last_subformula.lambda_0\n"
     ]
    }
   ],
   "source": [
    "simplified_formula = formula.simplify(X1, y1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle (\\frac{2}{1}x_1^{\\frac{1}{1}}(\\frac{4}{1}x_1^{\\frac{0}{1}}\\frac{-4}{1}) + (\\frac{4}{1}x_1^{\\frac{2}{1}} + \\frac{2}{1}))$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PrintFormula(simplified_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \\frac{1}{3}\n",
      "2 \\frac{1}{3}123\n"
     ]
    }
   ],
   "source": [
    "a = r\"\"\n",
    "a += r\"2 \\frac{1}{3}\"\n",
    "print(a)\n",
    "a += '123'\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = RecursiveFormula(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dict = a.state_dict()\n",
    "old_dict['lambda_0'] = torch.tensor([[179]])\n",
    "a.load_state_dict(old_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "Cff80iXMCq2P",
    "outputId": "ca2ef819-3ec8-4588-91a9-d86ccc177789"
   },
   "outputs": [],
   "source": [
    "PrintFormula(LearnFormula(X2, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2kXALfPpCq2h"
   },
   "outputs": [],
   "source": [
    "MSELoss()(y1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in formula.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "demonstration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
