{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q41R-POjCq1e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9aiZJrHCq1p"
   },
   "outputs": [],
   "source": [
    "class RecursiveFormula(nn.Module):\n",
    "    \"\"\"\n",
    "    Class used for representing formulas\n",
    "    \n",
    "    Attributes:\n",
    "        depth\n",
    "        num_variables\n",
    "        powers\n",
    "        lambdas - list of linear coefficients\n",
    "        subformulas - list of subformulas of smaller depth, which are used for computing\n",
    "        parameters - list of all learnable parameters of formula\n",
    "        last_subformula - additional subformula, which is not multiplied by variable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, depth=0, num_variables=1):\n",
    "        super(RecursiveFormula, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.num_variables = num_variables\n",
    "        self.powers = []\n",
    "        self.lambdas = []\n",
    "        self.subformulas = nn.ModuleList()\n",
    "        # When depth is zero, formula is just a real number\n",
    "        if depth == 0:\n",
    "            new_lambda = nn.Parameter((2 * torch.randn((1, 1)))).requires_grad_(True)\n",
    "            self.lambdas.append(new_lambda)\n",
    "            self.register_parameter(\"lambda_const_{}\".format(depth), new_lambda)\n",
    "        else:\n",
    "            for i in range(self.num_variables):\n",
    "                # When depth is 1, we do not need to create subformulas, since they would be just real numbers\n",
    "                if self.depth != 1:\n",
    "                    subformula = RecursiveFormula(self.depth - 1, self.num_variables)\n",
    "                    self.subformulas.append(subformula)\n",
    "                new_lambda = nn.Parameter((2 * torch.randn((1, 1)))).requires_grad_(True)\n",
    "                new_power = nn.Parameter((2 * torch.randn((1, 1)))).requires_grad_(True)\n",
    "                self.register_parameter(\"lambda_{}\".format(i), new_lambda)\n",
    "                self.register_parameter(\"power_{}\".format(i), new_power)\n",
    "                self.lambdas.append(new_lambda)\n",
    "                self.powers.append(new_power)\n",
    "            self.last_subformula = RecursiveFormula(self.depth - 1, self.num_variables)\n",
    "                                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Iterate over subformulas, recursively computing result using results of subformulas\n",
    "        \"\"\"\n",
    "        # When depth is 0, we just return the corresponding number\n",
    "        if self.depth == 0:\n",
    "            return self.lambdas[0].repeat(x.shape[0], 1)\n",
    "        \n",
    "        ans = torch.zeros(x.shape[0], 1)\n",
    "        for i in range(self.num_variables):\n",
    "            x_powered = torch.t(x[:, i]**self.powers[i])\n",
    "            subformula_result = torch.ones((x.shape[0], 1))\n",
    "            # When depth is 1, we do not need to compute subformulas\n",
    "            if self.depth != 1:\n",
    "                subformula_result = self.subformulas[i](x)\n",
    "            assert subformula_result.shape == (x.shape[0], 1)\n",
    "            assert x_powered.shape == (x.shape[0], 1)\n",
    "            ans += self.lambdas[i] * x_powered * subformula_result\n",
    "            \n",
    "        ans += self.last_subformula(x)\n",
    "        return ans\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return tex-style string, recursively combining result from representation of subformulas\n",
    "        \"\"\"\n",
    "        if self.depth == 0:\n",
    "            return \"{:.3}\".format(self.lambdas[0].item())\n",
    "        \n",
    "        ans = [\"(\"]\n",
    "        for i in range(self.num_variables):\n",
    "            if i != 0 and self.lambdas[i] > 0:\n",
    "                ans.append(\" + {:.3}\".format(self.lambdas[i].item()))\n",
    "            else:\n",
    "                ans.append(\"{:.3}\".format(self.lambdas[i].item()))            \n",
    "            ans.append(\"x_{}^\".format(i + 1) + \"{\" + \"{:.3}\".format(self.powers[i].item()) + \"}\")\n",
    "            if self.depth != 1:\n",
    "                ans.append(str(self.subformulas[i]))\n",
    "        if self.last_subformula.lambdas[0] > 0:        \n",
    "            ans.append(\" + \")\n",
    "        ans.append(str(self.last_subformula))\n",
    "        ans.append(\")\")\n",
    "        ans = ''.join(ans)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMZsInkrCq1w"
   },
   "outputs": [],
   "source": [
    "def info(formula):\n",
    "    print(\"depth: {}, number of variables: {}, total parameters: {}\".format(\n",
    "        formula.depth, formula.num_variables, len(formula.parameters)))\n",
    "    \n",
    "def PrintFormula(formula, mode=\"slow\"):\n",
    "#     info(network)\n",
    "    if mode == \"slow\":\n",
    "        display(Math(str(formula)))   \n",
    "    else:\n",
    "        print(formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8fgpfjNCq14"
   },
   "source": [
    "Обучение\n",
    "\n",
    "Пока модель быстро входит в локальный экстремум, и результат сильно зависит от начальной инициализации весов. До последнего свободного члена градиент не доходит. Буду пробовать разные оптимизаторы и гиперпараметры. Возможно, параметры powers и lambdas имеют разную природу, и надо их регуляризовать по отдельности с разными коэффициентами, которые тоже надо будет подобрать. Модели может быть выгодно просто делать степени большими по модулю отрицательными чистами. Еще надо добавить обучение по батчам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aTTheLOCq17"
   },
   "outputs": [],
   "source": [
    "def LearnFormula(X, y, depth=1, num_epochs=700, verbose=True, seed=13337):\n",
    "#     torch.random.manual_seed(seed)\n",
    "    formula = RecursiveFormula(depth, X.shape[1])\n",
    "    # create your optimizer\n",
    "    optimizer = optim.Rprop(formula.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = formula(X)\n",
    "        loss = criterion(output, y) \n",
    "        loss.backward()\n",
    "        if verbose and (epoch + 1) % 20 == 0:\n",
    "            print(\"Epoch {}, current loss {:.3}, current formula \".format(epoch + 1, loss.item()), end='')\n",
    "            PrintFormula(formula, \"fast\")       \n",
    "        optimizer.step()  \n",
    "    return formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_-H5b-ICq2A"
   },
   "outputs": [],
   "source": [
    "X1 = torch.rand(100, 1) * 10\n",
    "y1 = 2.5 * X1**2 + 3\n",
    "\n",
    "X2 = torch.rand(100, 2) * 10\n",
    "y2 = 2.5 * X2[:, 0]**2 + 0.333 * X2[:, 1]**0.5 + 3    \n",
    "# y = 1.2 * X[:, 0]**2.1 * X[:, 2] + 2.5 * X[:, 1]**(-3) + 1/2 * X[:, 2]**0.3333 * X[:, 1]**(-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "rlfMCIBjNBgo",
    "outputId": "824c5037-a5a3-4729-ac47-83ff5838e7d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, current loss 9.83e+03, current formula (0.579x_1^{-0.982}-2.67)\n",
      "Epoch 40, current loss 4.42e+02, current formula (3.31x_1^{1.75} + 0.0637)\n",
      "Epoch 60, current loss 3.49, current formula (3.42x_1^{1.86} + 0.167)\n",
      "Epoch 80, current loss 3.15, current formula (3.39x_1^{1.86}-0.268)\n",
      "Epoch 100, current loss 2.72, current formula (3.32x_1^{1.87}-0.0316)\n",
      "Epoch 120, current loss 2.64, current formula (3.31x_1^{1.87}-0.00988)\n",
      "Epoch 140, current loss 2.33, current formula (3.25x_1^{1.88} + 0.149)\n",
      "Epoch 160, current loss 2.0, current formula (3.19x_1^{1.89} + 0.409)\n",
      "Epoch 180, current loss 1.76, current formula (3.14x_1^{1.9} + 0.541)\n",
      "Epoch 200, current loss 1.58, current formula (3.11x_1^{1.9} + 0.674)\n",
      "Epoch 220, current loss 1.34, current formula (3.06x_1^{1.91} + 0.927)\n",
      "Epoch 240, current loss 1.19, current formula (3.02x_1^{1.91} + 0.959)\n",
      "Epoch 260, current loss 1.04, current formula (2.98x_1^{1.92} + 1.19)\n",
      "Epoch 280, current loss 1.01, current formula (2.98x_1^{1.92} + 1.18)\n",
      "Epoch 300, current loss 0.909, current formula (2.95x_1^{1.93} + 1.24)\n",
      "Epoch 320, current loss 0.812, current formula (2.92x_1^{1.93} + 1.31)\n",
      "Epoch 340, current loss 0.745, current formula (2.9x_1^{1.93} + 1.48)\n",
      "Epoch 360, current loss 0.693, current formula (2.89x_1^{1.93} + 1.5)\n",
      "Epoch 380, current loss 0.625, current formula (2.87x_1^{1.94} + 1.53)\n",
      "Epoch 400, current loss 0.557, current formula (2.85x_1^{1.94} + 1.65)\n",
      "Epoch 420, current loss 0.501, current formula (2.83x_1^{1.94} + 1.71)\n",
      "Epoch 440, current loss 0.442, current formula (2.81x_1^{1.95} + 1.79)\n",
      "Epoch 460, current loss 0.396, current formula (2.79x_1^{1.95} + 1.87)\n",
      "Epoch 480, current loss 0.359, current formula (2.78x_1^{1.95} + 1.89)\n",
      "Epoch 500, current loss 0.321, current formula (2.76x_1^{1.96} + 2.0)\n",
      "Epoch 520, current loss 0.294, current formula (2.75x_1^{1.96} + 2.01)\n",
      "Epoch 540, current loss 0.267, current formula (2.74x_1^{1.96} + 2.06)\n",
      "Epoch 560, current loss 0.242, current formula (2.72x_1^{1.96} + 2.13)\n",
      "Epoch 580, current loss 0.219, current formula (2.71x_1^{1.96} + 2.13)\n",
      "Epoch 600, current loss 0.196, current formula (2.7x_1^{1.97} + 2.2)\n",
      "Epoch 620, current loss 0.179, current formula (2.69x_1^{1.97} + 2.23)\n",
      "Epoch 640, current loss 0.159, current formula (2.68x_1^{1.97} + 2.28)\n",
      "Epoch 660, current loss 0.144, current formula (2.67x_1^{1.97} + 2.32)\n",
      "Epoch 680, current loss 0.13, current formula (2.66x_1^{1.97} + 2.34)\n",
      "Epoch 700, current loss 0.116, current formula (2.65x_1^{1.97} + 2.38)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle (2.65x_1^{1.97} + 2.38)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PrintFormula(LearnFormula(X1, y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "Cff80iXMCq2P",
    "outputId": "ca2ef819-3ec8-4588-91a9-d86ccc177789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, current loss 1.39e+04, current formula (-0.635x_1^{-1.12}-0.478x_2^{1.75}-0.466)\n",
      "Epoch 40, current loss 6.52e+03, current formula (2.04x_1^{1.55} + 1.5x_2^{1.68} + 5.5)\n",
      "Epoch 60, current loss 5.12e+03, current formula (1.7x_1^{1.21} + 1.53x_2^{0.667} + 63.8)\n",
      "Epoch 80, current loss 5.05e+03, current formula (1.05x_1^{-0.0235} + 1.54x_2^{-0.027} + 79.9)\n",
      "Epoch 100, current loss 5.05e+03, current formula (1.05x_1^{-0.0114} + 1.54x_2^{0.0136} + 79.7)\n",
      "Epoch 120, current loss 5.05e+03, current formula (1.05x_1^{-0.000689} + 1.54x_2^{-0.000293} + 79.7)\n",
      "Epoch 140, current loss 5.05e+03, current formula (1.05x_1^{5.35e-05} + 1.54x_2^{3.74e-06} + 79.7)\n",
      "Epoch 160, current loss 5.05e+03, current formula (1.05x_1^{1.24e-06} + 1.54x_2^{5.94e-06} + 79.7)\n",
      "Epoch 180, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 200, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 220, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 240, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 260, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 280, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 300, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 320, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 340, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 360, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 380, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 400, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 420, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 440, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 460, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 480, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 500, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 520, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 540, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 560, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 580, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 600, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 620, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 640, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 660, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 680, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n",
      "Epoch 700, current loss 5.05e+03, current formula (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle (1.05x_1^{2.44e-06} + 1.54x_2^{-1.63e-06} + 79.7)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PrintFormula(LearnFormula(X2, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2kXALfPpCq2h"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "demonstration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
