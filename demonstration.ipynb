{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q41R-POjCq1e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import MSELoss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_length_of_fraction(numerator, denominator):\n",
    "    return torch.log2((1 + torch.abs(torch.tensor(denominator))) * torch.abs(torch.tensor(numerator)))\n",
    "\n",
    "\n",
    "def logplus(number):\n",
    "    return torch.log2(1 + number**2) / 2\n",
    "\n",
    "\n",
    "def descriptive_length_of_real_number(real_number, precision_floor=1e-8):\n",
    "    return logplus(real_number / torch.tensor(precision_floor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(24.9905)\n"
     ]
    }
   ],
   "source": [
    "print(descriptive_length_of_fraction(1.0, 3.0))\n",
    "print(descriptive_length_of_real_number(0.3333333345))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FormFractionRepresentation(fraction: torch.tensor) -> str:\n",
    "    if fraction[1].item() != 1:\n",
    "        return r\"\\frac{\" + str(int(fraction[0].item())) + \"}{\" + str(int(fraction[1].item())) + \"}\"\n",
    "    return str(int(fraction[0].item()))\n",
    "\n",
    "\n",
    "def FormReal(number: torch.tensor) -> str:\n",
    "    return \"{:.3}\".format(number.item())\n",
    "\n",
    "\n",
    "def AddRationalInName(name: str) -> str:\n",
    "    if 'lambda' in name:\n",
    "        position = name.find('lambda')\n",
    "    else:\n",
    "        position = name.find('power')\n",
    "    return name[:position] + \"rational_\" + name[position:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMZsInkrCq1w"
   },
   "outputs": [],
   "source": [
    "def info(formula):\n",
    "    print(\"depth: {}, number of variables: {}, total parameters: {}\".format(\n",
    "        formula.depth, formula.num_variables, len(formula.parameters)))\n",
    "    \n",
    "def PrintFormula(formula, mode=\"slow\"):\n",
    "#     info(network)\n",
    "    if mode == \"slow\":\n",
    "        display(Math(str(formula)))   \n",
    "    else:\n",
    "        print(formula)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9aiZJrHCq1p"
   },
   "outputs": [],
   "source": [
    "class RecursiveFormula(nn.Module):\n",
    "    \"\"\"\n",
    "    Class used for representing formulas\n",
    "    \n",
    "    Attributes:\n",
    "        depth\n",
    "        num_variables\n",
    "        subformulas - list of subformulas of smaller depth, which are used for computing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, depth=0, num_variables=1):\n",
    "        super(RecursiveFormula, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.num_variables = num_variables\n",
    "        self.subformulas = nn.ModuleList()\n",
    "        # When depth is zero, formula is just a real number\n",
    "        if depth == 0:\n",
    "            new_lambda = nn.Parameter((2 * torch.randn((1, 1)))).requires_grad_(True)\n",
    "            self.register_parameter(\"lambda_0\", new_lambda)\n",
    "            new_rational_lambda = nn.Parameter(torch.tensor([0., 0.])).requires_grad_(False)\n",
    "            self.register_parameter(\"rational_lambda_0\", new_rational_lambda)\n",
    "        else:\n",
    "            for i in range(self.num_variables):\n",
    "                # When depth is 1, we do not need to create subformulas, since they would be just real numbers\n",
    "                if self.depth != 1:\n",
    "                    subformula = RecursiveFormula(self.depth - 1, self.num_variables)\n",
    "                    self.subformulas.append(subformula)\n",
    "                new_lambda = nn.Parameter((2 * torch.randn((1, 1)))).requires_grad_(True)\n",
    "                new_power = nn.Parameter((2 * torch.randn((1, 1)))).requires_grad_(True)\n",
    "                new_rational_lambda = nn.Parameter(torch.tensor([0., 0.])).requires_grad_(False)\n",
    "                new_rational_power = nn.Parameter(torch.tensor([0., 0.])).requires_grad_(False)\n",
    "                self.register_parameter(\"lambda_{}\".format(i), new_lambda)\n",
    "                self.register_parameter(\"power_{}\".format(i), new_power)\n",
    "                self.register_parameter(\"rational_lambda_{}\".format(i), new_rational_lambda)\n",
    "                self.register_parameter(\"rational_power_{}\".format(i), new_rational_power)\n",
    "            self.last_subformula = RecursiveFormula(self.depth - 1, self.num_variables)\n",
    "                                    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Iterate over subformulas, recursively computing result using results of subformulas\n",
    "        \"\"\"\n",
    "        # When depth is 0, we just return the corresponding number\n",
    "        if self.depth == 0:\n",
    "            return self.get_lambda(0).repeat(x.shape[0], 1)\n",
    "        \n",
    "        ans = torch.zeros(x.shape[0], 1)\n",
    "        for i in range(self.num_variables):\n",
    "            x_powered = torch.t(x[:, i]**self.get_power(i))\n",
    "            subformula_result = torch.ones((x.shape[0], 1))\n",
    "            # When depth is 1, we do not need to compute subformulas\n",
    "            if self.depth != 1:\n",
    "                subformula_result = self.subformulas[i](x)\n",
    "            ans += self.get_lambda(i) * x_powered * subformula_result           \n",
    "        ans += self.last_subformula(x)\n",
    "        return ans\n",
    "    \n",
    "    def simplify(self, X_val, y_val, max_denominator=10, inplace=False):\n",
    "        \"\"\"\n",
    "        Simplifies the formula, iterating over all its parameters and trying to substitute them with close rational number\n",
    "        \n",
    "        Parameters:\n",
    "            X_val: torch.tensor, shape (n_samples, n_features)\n",
    "                Training vector, where n_samples is the number of samples and n_features is the number of features.\n",
    "            y_val: torch.tensor, shape (n_samples, 1)\n",
    "                Target vector relative to X.\n",
    "            max_denominator: int\n",
    "                algorithm tries rational numbers with denominator not greater than max_denominator\n",
    "            inplace: bool\n",
    "                if True, when modify the original formula\n",
    "                otherwise return new formula, leaving original one unchanged\n",
    "        Returns:\n",
    "            self, if inplace set to True\n",
    "            simplified_version otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        simplified_version = copy.deepcopy(self)  \n",
    "        simplified_state_dict = simplified_version.state_dict()\n",
    "        \n",
    "        # Iterate over all parameters\n",
    "        for key, value in self.state_dict().items():\n",
    "            if \"rational\" not in key: # We do not simplify rational parameters - they will be the result of simplification\n",
    "                simplified_version_for_iteration = copy.deepcopy(simplified_version)\n",
    "                simplified_state_dict_for_iteration = simplified_version_for_iteration.state_dict()\n",
    "                y_predict = simplified_version(X_val)\n",
    "                loss = MSELoss()(y_val, y_predict)\n",
    "                descriptive_length_of_loss = descriptive_length_of_real_number(loss)\n",
    "                descriptive_length_of_existing_parameter = descriptive_length_of_real_number(value)\n",
    "\n",
    "                # Iterate over all possible denominators\n",
    "                for possible_denominator in range(1, max_denominator + 1):\n",
    "#                     print(\"trying denominator\", possible_denominator)\n",
    "                    simplified_parameter_numerator = torch.round(value * possible_denominator)\n",
    "                    simplified_state_dict_for_iteration[key] = simplified_parameter_numerator / possible_denominator\n",
    "                    simplified_version_for_iteration.load_state_dict(simplified_state_dict_for_iteration)\n",
    "                    descriptive_length_of_simplified_parameter = descriptive_length_of_fraction(simplified_parameter_numerator, possible_denominator)\n",
    "#                     print(simplified_parameter_numerator, possible_denominator)\n",
    "                    y_predict_simplified = simplified_version_for_iteration(X_val)\n",
    "                    loss_of_simplified_model = MSELoss()(y_val, y_predict_simplified)\n",
    "                    descriptive_length_of_loss_of_simplified_model = descriptive_length_of_real_number(loss_of_simplified_model)                \n",
    "                    # If the descriptive length did not improve, revert the change.\n",
    "#                     print(\"descriptive_length_of_loss_of_simplified_model\", descriptive_length_of_loss_of_simplified_model)\n",
    "#                     print(\"descriptive_length_of_simplified_parameter\", descriptive_length_of_simplified_parameter)\n",
    "#                     print(\"descriptive_length_of_loss\", descriptive_length_of_loss)\n",
    "#                     print(\"descriptive_length_of_existing_parameter\", descriptive_length_of_existing_parameter)\n",
    "\n",
    "                    if descriptive_length_of_loss_of_simplified_model + descriptive_length_of_simplified_parameter > descriptive_length_of_loss + descriptive_length_of_existing_parameter:\n",
    "                        simplified_version_for_iteration.load_state_dict(simplified_state_dict)\n",
    "                    else:\n",
    "                        # If we are successful, we update everything\n",
    "                        simplified_state_dict[AddRationalInName(key)] = torch.tensor([simplified_parameter_numerator, possible_denominator])\n",
    "                        simplified_version.load_state_dict(simplified_state_dict)\n",
    "                        simplified_version_for_iteration = copy.deepcopy(simplified_version)\n",
    "                        simplified_state_dict_for_iteration = simplified_version_for_iteration.state_dict()\n",
    "\n",
    "                simplified_state_dict = simplified_state_dict_for_iteration\n",
    "                simplified_version.load_state_dict(simplified_state_dict)\n",
    "        \n",
    "        if inplace:\n",
    "            self = copy.deepcopy(simplified_version)\n",
    "        else:\n",
    "            return simplified_version      \n",
    "    \n",
    "    def get_lambda(self, i):\n",
    "        return self.__getattr__('lambda_{}'.format(i))\n",
    "    \n",
    "    def get_rational_lambda(self, i):\n",
    "        return self.__getattr__('rational_lambda_{}'.format(i))\n",
    "    \n",
    "    def get_power(self, i):\n",
    "        return self.__getattr__('power_{}'.format(i))\n",
    "    \n",
    "    def get_rational_power(self, i):\n",
    "        return self.__getattr__('rational_power_{}'.format(i))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return tex-style string, recursively combining result from representation of subformulas\n",
    "        \"\"\"\n",
    "        if self.depth == 0:\n",
    "            if self.get_rational_lambda(0)[1] > 0: # if it is equal to 0, it means that there is no rational value\n",
    "                return FormFractionRepresentation(self.get_rational_lambda(0))            \n",
    "            return FormReal(self.get_lambda(0))\n",
    "        \n",
    "        ans = [\"(\"]\n",
    "        for i in range(self.num_variables):\n",
    "            # First we add lambda\n",
    "            if i != 0 and self.get_lambda(i) > 0:\n",
    "                ans.append(\" + \")\n",
    "            if self.get_rational_lambda(i)[1] > 0:\n",
    "                ans.append(FormFractionRepresentation(self.get_rational_lambda(i)))\n",
    "            else:\n",
    "                ans.append(FormReal(self.get_lambda(i)))   \n",
    "            # Then we add variable and its power\n",
    "            ans.append(\"x_{}^\".format(i + 1) + \"{\")\n",
    "            if self.get_rational_power(i)[1] > 0:\n",
    "                ans.append(FormFractionRepresentation(self.get_rational_power(i)))\n",
    "            else:\n",
    "                ans.append(FormReal(self.get_power(i)))  \n",
    "            ans += \"}\"    \n",
    "            # Then we add the corresponding subformula\n",
    "            if self.depth != 1:\n",
    "                ans.append(str(self.subformulas[i]))\n",
    "        if self.last_subformula.get_lambda(0) > 0:        \n",
    "            ans.append(\" + \")\n",
    "        ans.append(str(self.last_subformula))\n",
    "        ans.append(\")\")\n",
    "        ans = ''.join(ans)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aTTheLOCq17"
   },
   "outputs": [],
   "source": [
    "def LearnFormula(X, y, n_init=10, max_iter=100000, depth=1, verbose=2, verbose_frequency=5000, max_epochs_without_improvement=500,\n",
    "                minimal_acceptable_improvement=3e-6) -> RecursiveFormula:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        X: torch.tensor, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and n_features is the number of features.\n",
    "        y: torch.tensor, shape (n_samples, 1)\n",
    "            Target vector relative to X.\n",
    "        n_init: int \n",
    "            number of times algorithm will be run with different initial weights. \n",
    "            The final results will be the best output of n_init consecutive runs in terms of loss.\n",
    "        max_iter: int \n",
    "            Maximum number of iterations of the algorithm for a single run.\n",
    "        depth: int\n",
    "            depth of formula to learn\n",
    "        verbose: int\n",
    "            if is equal to 0, no output\n",
    "            if is equal to 1, output number of runs and losses\n",
    "            if is equal to 2, output number of runs and losses and print loss every verbose_frequency epochs\n",
    "        verbose_frequency: int\n",
    "            if verbose equals 2, then print loss every verbose_frequency epochs\n",
    "        max_epochs_without_improvement: int\n",
    "            if during this number of epochs loss does not decrease more than minimal_acceptable_improvement, the learning process\n",
    "            will be finished\n",
    "        minimal_acceptable_improvement: float\n",
    "            if during max_epochs_without_improvement number of epochs loss does not decrease more than this number, \n",
    "            the learning process will be finished\n",
    "            \n",
    "    Returns:\n",
    "        best_formula: RecursiveFormula\n",
    "            fitted formula\n",
    "    \"\"\"\n",
    "    \n",
    "    best_model = RecursiveFormula(depth, X.shape[1])\n",
    "    best_loss = 1e10\n",
    "    for init in range(n_init):\n",
    "        if verbose > 0:\n",
    "            print(\"Run #{}\".format(init + 1))\n",
    "    #     torch.random.manual_seed(seed)\n",
    "        formula = RecursiveFormula(depth, X.shape[1])\n",
    "        # create your optimizer\n",
    "        optimizer = optim.Adam(formula.parameters(), weight_decay=1)\n",
    "        criterion = nn.MSELoss()\n",
    "        epochs_without_improvement = 0\n",
    "        epoch = 0\n",
    "        optimizer.zero_grad()\n",
    "        output = formula(X)\n",
    "        previous_loss = criterion(output, y).item() \n",
    "        while epoch < max_iter and epochs_without_improvement < max_epochs_without_improvement:\n",
    "            optimizer.zero_grad()\n",
    "            output = formula(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            if verbose == 2 and (epoch + 1) % verbose_frequency == 0:\n",
    "                print(\"Epoch {}, current loss {:.3}, current formula \".format(epoch + 1, loss.item()), end='')\n",
    "                PrintFormula(formula, \"fast\")       \n",
    "            optimizer.step()  \n",
    "            epoch += 1\n",
    "            if torch.abs(previous_loss - loss) < minimal_acceptable_improvement:\n",
    "                epochs_without_improvement += 1\n",
    "            else:\n",
    "                epochs_without_improvement = 0\n",
    "            previous_loss = loss.item()\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_formula = formula\n",
    "        if verbose > 0:\n",
    "            print(\"Finished run #{}, loss {}, best loss {}\".format(init + 1, loss, best_loss))\n",
    "    return best_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_-H5b-ICq2A"
   },
   "outputs": [],
   "source": [
    "X1 = torch.rand(100, 1) * 10\n",
    "y1 = 2.5 * X1**2 + 3\n",
    "\n",
    "X2 = torch.rand(100, 2) * 10\n",
    "y2 = 2.5 * X2[:, 0]**2 + 0.3333 * X2[:, 1]**0.25 + 3    \n",
    "# y = 1.2 * X[:, 0]**2.1 * X[:, 2] + 2.5 * X[:, 1]**(-3) + 1/2 * X[:, 2]**0.3333 * X[:, 1]**(-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #1\n",
      "Epoch 5000, current loss 55.4, current formula (1.26x_1^{2.33} + 1.74)\n",
      "Epoch 10000, current loss 1.41, current formula (2.32x_1^{2.04} + 2.34)\n",
      "Finished run #1, loss 0.8252627849578857, best loss 0.8252627849578857\n",
      "Run #2\n",
      "Epoch 5000, current loss 1.75, current formula (2.89x_1^{1.94} + 0.0967)\n",
      "Epoch 10000, current loss 0.833, current formula (2.69x_1^{1.97} + 1.1)\n",
      "Finished run #2, loss 0.8254114985466003, best loss 0.8252627849578857\n",
      "Run #3\n",
      "Epoch 5000, current loss 8.37, current formula (1.82x_1^{2.15} + 3.57)\n",
      "Epoch 10000, current loss 1.97, current formula (2.16x_1^{2.07} + 3.22)\n",
      "Epoch 15000, current loss 0.728, current formula (2.66x_1^{1.98} + 1.27)\n",
      "Finished run #3, loss 0.8252681493759155, best loss 0.8252627849578857\n",
      "Run #4\n",
      "Epoch 5000, current loss 0.803, current formula (2.46x_1^{2.01} + 1.93)\n",
      "Finished run #4, loss 0.8252605199813843, best loss 0.8252605199813843\n",
      "Run #5\n",
      "Epoch 5000, current loss 52.0, current formula (1.35x_1^{2.3} + 1.13)\n",
      "Epoch 10000, current loss 13.8, current formula (1.82x_1^{2.15} + 2.17)\n",
      "Epoch 15000, current loss 0.725, current formula (2.59x_1^{1.99} + 1.46)\n",
      "Finished run #5, loss 0.8252529501914978, best loss 0.8252529501914978\n",
      "Run #6\n",
      "Epoch 5000, current loss 9.87, current formula (3.19x_1^{1.88} + 3.92)\n",
      "Epoch 10000, current loss 0.413, current formula (2.58x_1^{1.99} + 1.81)\n",
      "Finished run #6, loss 0.8252724409103394, best loss 0.8252529501914978\n",
      "Run #7\n",
      "Epoch 5000, current loss 1.09e+04, current formula (1.54x_1^{0.288} + 7.2)\n",
      "Epoch 10000, current loss 5.57, current formula (2.53x_1^{1.98} + 6.11)\n",
      "Epoch 15000, current loss 0.109, current formula (2.43x_1^{2.01} + 2.94)\n",
      "Epoch 20000, current loss 0.82, current formula (2.69x_1^{1.97} + 1.12)\n",
      "Finished run #7, loss 0.8253172039985657, best loss 0.8252529501914978\n",
      "Run #8\n",
      "Epoch 5000, current loss 1.17e+04, current formula (-0.0022x_1^{-4.18} + 4.78)\n",
      "Epoch 10000, current loss 1.09e+04, current formula (-0.000664x_1^{-4.18} + 9.49)\n",
      "Epoch 15000, current loss 1.02e+04, current formula (-0.00117x_1^{-4.18} + 14.4)\n",
      "Epoch 20000, current loss 9.57e+03, current formula (-0.0017x_1^{-4.18} + 19.3)\n",
      "Epoch 25000, current loss 8.96e+03, current formula (-0.00264x_1^{-4.1} + 24.1)\n",
      "Epoch 30000, current loss 8.39e+03, current formula (-0.0274x_1^{-3.14} + 29.0)\n",
      "Epoch 35000, current loss 7.86e+03, current formula (-1.58x_1^{-1.46} + 33.8)\n",
      "Epoch 40000, current loss 7.39e+03, current formula (-2.25x_1^{-1.38} + 38.6)\n",
      "Epoch 45000, current loss 6.96e+03, current formula (-3.04x_1^{-1.3} + 43.3)\n",
      "Epoch 50000, current loss 6.58e+03, current formula (-3.97x_1^{-1.24} + 48.0)\n",
      "Epoch 55000, current loss 6.24e+03, current formula (-5.02x_1^{-1.18} + 52.5)\n",
      "Epoch 60000, current loss 5.97e+03, current formula (-6.09x_1^{-1.13} + 56.5)\n",
      "Epoch 65000, current loss 5.83e+03, current formula (-6.73x_1^{-1.11} + 58.8)\n",
      "Epoch 70000, current loss 5.83e+03, current formula (-6.74x_1^{-1.11} + 58.8)\n",
      "Epoch 75000, current loss 5.83e+03, current formula (-6.74x_1^{-1.11} + 58.8)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5b19f1efe267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mformula1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearnFormula\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-b4a2bfea027c>\u001b[0m in \u001b[0;36mLearnFormula\u001b[0;34m(X, y, n_init, max_iter, depth, verbose, verbose_frequency, max_epochs_without_improvement, minimal_acceptable_improvement)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformula\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mverbose_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}, current loss {:.3}, current formula \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "formula1 = LearnFormula(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_formula1 = formula.simplify(X1, y1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintFormula(simplified_formula1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #1\n",
      "Epoch 5000, current loss 5.96e+03, current formula (2.73x_1^{1.39} + 4.52x_2^{1.15} + 6.39)\n",
      "Epoch 10000, current loss 5.54e+03, current formula (5.27x_1^{1.02} + 8.6x_2^{0.811} + 10.8)\n",
      "Epoch 15000, current loss 5.26e+03, current formula (9.14x_1^{0.717} + 12.5x_2^{0.584} + 14.6)\n",
      "Epoch 20000, current loss 5.15e+03, current formula (12.7x_1^{0.547} + 14.9x_2^{0.47} + 15.3)\n",
      "Epoch 25000, current loss 5.13e+03, current formula (14.6x_1^{0.494} + 14.6x_2^{0.467} + 14.6)\n",
      "Epoch 30000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Finished run #1, loss 5134.796875, best loss 5134.796875\n",
      "Run #2\n",
      "Epoch 5000, current loss 7.34e+03, current formula (4.05x_1^{1.47} + 0.619x_2^{-1.89} + 0.481)\n",
      "Epoch 10000, current loss 6.53e+03, current formula (8.24x_1^{1.12} + 0.167x_2^{-1.75} + 5.16)\n",
      "Epoch 15000, current loss 6.05e+03, current formula (12.6x_1^{0.889} + 0.167x_2^{-1.61} + 9.79)\n",
      "Epoch 20000, current loss 5.32e+03, current formula (16.3x_1^{0.551} + 3.91x_2^{1.02} + 13.7)\n",
      "Epoch 25000, current loss 5.19e+03, current formula (16.8x_1^{0.48} + 7.9x_2^{0.696} + 15.9)\n",
      "Epoch 30000, current loss 5.16e+03, current formula (15.3x_1^{0.491} + 11.9x_2^{0.542} + 15.3)\n",
      "Epoch 35000, current loss 5.14e+03, current formula (14.6x_1^{0.493} + 14.5x_2^{0.47} + 14.7)\n",
      "Epoch 40000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Finished run #2, loss 5134.796875, best loss 5134.796875\n",
      "Run #3\n",
      "Epoch 5000, current loss 6.95e+03, current formula (-2.81x_1^{-1.92} + 6.47x_2^{1.24} + 5.12)\n",
      "Epoch 10000, current loss 6.2e+03, current formula (-1.23x_1^{-1.37} + 10.6x_2^{0.953} + 9.74)\n",
      "Epoch 15000, current loss 5.76e+03, current formula (1.75x_1^{-1.03} + 15.0x_2^{0.742} + 14.3)\n",
      "Epoch 20000, current loss 5.21e+03, current formula (6.42x_1^{0.817} + 16.8x_2^{0.469} + 16.4)\n",
      "Epoch 25000, current loss 5.17e+03, current formula (10.5x_1^{0.618} + 15.7x_2^{0.464} + 15.7)\n",
      "Epoch 30000, current loss 5.14e+03, current formula (13.9x_1^{0.511} + 14.8x_2^{0.467} + 14.8)\n",
      "Epoch 35000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Epoch 40000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Finished run #3, loss 5134.796875, best loss 5134.796875\n",
      "Run #4\n",
      "Epoch 5000, current loss 6.06e+03, current formula (3.11x_1^{1.35} + 4.86x_2^{1.15} + 2.34)\n",
      "Epoch 10000, current loss 5.6e+03, current formula (6.49x_1^{0.959} + 8.9x_2^{0.812} + 6.58)\n",
      "Epoch 15000, current loss 5.3e+03, current formula (10.4x_1^{0.687} + 12.8x_2^{0.589} + 10.8)\n",
      "Epoch 20000, current loss 5.15e+03, current formula (13.7x_1^{0.523} + 15.1x_2^{0.466} + 14.0)\n",
      "Epoch 25000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Epoch 30000, current loss 5.13e+03, current formula (14.6x_1^{0.494} + 14.6x_2^{0.467} + 14.6)\n",
      "Epoch 35000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Finished run #4, loss 5134.796875, best loss 5134.796875\n",
      "Run #5\n",
      "Epoch 5000, current loss 7.11e+03, current formula (4.12x_1^{1.48}-1.28x_2^{-1.24} + 3.49)\n",
      "Epoch 10000, current loss 6.38e+03, current formula (8.13x_1^{1.11} + 0.363x_2^{-1.04} + 8.11)\n",
      "Epoch 15000, current loss 5.45e+03, current formula (11.8x_1^{0.7} + 4.49x_2^{1.02} + 12.1)\n",
      "Epoch 20000, current loss 5.22e+03, current formula (14.9x_1^{0.536} + 8.41x_2^{0.693} + 15.2)\n",
      "Epoch 25000, current loss 5.15e+03, current formula (15.3x_1^{0.49} + 12.2x_2^{0.533} + 15.3)\n",
      "Epoch 30000, current loss 5.14e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.469} + 14.7)\n",
      "Epoch 35000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Finished run #5, loss 5134.796875, best loss 5134.796875\n",
      "Run #6\n",
      "Epoch 5000, current loss 7.6e+03, current formula (2.53x_1^{1.74} + 0.707x_2^{-0.808}-1.41)\n",
      "Epoch 10000, current loss 6.03e+03, current formula (2.72x_1^{1.41} + 5.62x_2^{1.08} + 2.85)\n",
      "Epoch 15000, current loss 5.6e+03, current formula (5.07x_1^{1.06} + 9.8x_2^{0.78} + 7.35)\n",
      "Epoch 20000, current loss 5.3e+03, current formula (8.95x_1^{0.741} + 13.7x_2^{0.57} + 11.5)\n",
      "Epoch 25000, current loss 5.15e+03, current formula (12.5x_1^{0.554} + 15.5x_2^{0.462} + 14.5)\n",
      "Epoch 30000, current loss 5.14e+03, current formula (14.6x_1^{0.494} + 14.6x_2^{0.467} + 14.7)\n",
      "Epoch 35000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Finished run #6, loss 5134.796875, best loss 5134.796875\n",
      "Run #7\n",
      "Epoch 5000, current loss 6.15e+03, current formula (3.66x_1^{1.3} + 2.93x_2^{1.36} + 2.2)\n",
      "Epoch 10000, current loss 5.64e+03, current formula (7.59x_1^{0.914} + 6.44x_2^{0.941} + 6.56)\n",
      "Epoch 15000, current loss 5.33e+03, current formula (11.5x_1^{0.664} + 10.4x_2^{0.668} + 10.8)\n",
      "Epoch 20000, current loss 5.16e+03, current formula (14.5x_1^{0.508} + 13.7x_2^{0.499} + 14.1)\n",
      "Epoch 25000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Finished run #7, loss 5134.796875, best loss 5134.796875\n",
      "Run #8\n",
      "Epoch 5000, current loss 5.97e+03, current formula (4.94x_1^{1.15} + 4.25x_2^{1.18} + 2.56)\n",
      "Epoch 10000, current loss 5.56e+03, current formula (8.98x_1^{0.827} + 7.67x_2^{0.851} + 6.7)\n",
      "Epoch 15000, current loss 5.28e+03, current formula (12.8x_1^{0.606} + 11.6x_2^{0.61} + 10.9)\n",
      "Epoch 20000, current loss 5.14e+03, current formula (14.9x_1^{0.492} + 14.4x_2^{0.477} + 14.0)\n",
      "Epoch 25000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Epoch 30000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Finished run #8, loss 5134.796875, best loss 5134.796875\n",
      "Run #9\n",
      "Epoch 5000, current loss 6.95e+03, current formula (3.61x_1^{1.48} + 0.429x_2^{-1.99} + 7.26)\n",
      "Epoch 10000, current loss 6.24e+03, current formula (7.72x_1^{1.11} + 0.0783x_2^{-1.91} + 11.8)\n",
      "Epoch 15000, current loss 5.81e+03, current formula (12.0x_1^{0.865} + 0.066x_2^{-1.83} + 16.3)\n",
      "Epoch 20000, current loss 5.42e+03, current formula (16.3x_1^{0.656} + 2.3x_2^{0.272} + 20.5)\n",
      "Epoch 25000, current loss 5.18e+03, current formula (16.5x_1^{0.485} + 6.83x_2^{0.743} + 17.8)\n",
      "Epoch 30000, current loss 5.16e+03, current formula (15.5x_1^{0.491} + 11.0x_2^{0.57} + 15.6)\n",
      "Epoch 35000, current loss 5.14e+03, current formula (14.7x_1^{0.493} + 14.2x_2^{0.477} + 14.7)\n",
      "Finished run #9, loss 5134.796875, best loss 5134.796875\n",
      "Run #10\n",
      "Epoch 5000, current loss 7.15e+03, current formula (0.958x_1^{2.06} + 0.0515x_2^{3.02} + 2.04)\n",
      "Epoch 10000, current loss 6.64e+03, current formula (2.12x_1^{1.6} + 0.0707x_2^{3.0} + 5.92)\n",
      "Epoch 15000, current loss 6.11e+03, current formula (5.54x_1^{1.14} + 0.0885x_2^{2.81} + 10.5)\n",
      "Epoch 20000, current loss 5.59e+03, current formula (9.57x_1^{0.821} + 1.22x_2^{1.58} + 14.8)\n",
      "Epoch 25000, current loss 5.28e+03, current formula (13.3x_1^{0.6} + 4.73x_2^{0.928} + 18.0)\n",
      "Epoch 30000, current loss 5.18e+03, current formula (15.9x_1^{0.494} + 8.73x_2^{0.658} + 16.4)\n",
      "Epoch 35000, current loss 5.15e+03, current formula (15.1x_1^{0.492} + 12.6x_2^{0.521} + 15.1)\n",
      "Epoch 40000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.468} + 14.6)\n",
      "Epoch 45000, current loss 5.13e+03, current formula (14.6x_1^{0.493} + 14.6x_2^{0.467} + 14.6)\n",
      "Finished run #10, loss 5134.796875, best loss 5134.796875\n"
     ]
    }
   ],
   "source": [
    "formula = LearnFormula(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikhail/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "simplified_formula = formula.simplify(X2, y2, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle (\\frac{146}{10}x_1^{\\frac{5}{10}} + \\frac{146}{10}x_2^{\\frac{5}{10}} + \\frac{146}{10})$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PrintFormula(simplified_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "demonstration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
